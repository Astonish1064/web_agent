"""
TDD Tests for TaskGenerator (Phase 1.1)

Tests the ITaskGenerator interface and LLMTaskGenerator implementation.
Following the PROMPT_TASK_GENERATION contract.
"""
import unittest
from unittest.mock import MagicMock, patch
from dataclasses import dataclass
from typing import List
import json

# Import the module under test (will fail initially - TDD Red)
import sys
sys.path.insert(0, '/volume/pt-coder/users/lysun/kzheng/web_agent/infiniteweb_repro')

from src.interfaces import ILLMProvider


@dataclass
class TaskConfig:
    """Configuration for task generation."""
    website_type: str
    task_count_min: int = 3
    task_count_max: int = 7
    min_steps: int = 3
    max_steps: int = 8


@dataclass 
class GeneratedTask:
    """A task generated by ITaskGenerator."""
    id: str
    name: str
    description: str
    steps: List[str]


class TestTaskGeneratorInterface(unittest.TestCase):
    """Tests for ITaskGenerator interface contract."""
    
    def test_interface_exists(self):
        """ITaskGenerator interface should be importable."""
        from src.interfaces import ITaskGenerator
        self.assertTrue(hasattr(ITaskGenerator, 'generate'))
    
    def test_interface_is_abstract(self):
        """ITaskGenerator should be abstract and not instantiable."""
        from src.interfaces import ITaskGenerator
        with self.assertRaises(TypeError):
            ITaskGenerator()


class TestLLMTaskGenerator(unittest.TestCase):
    """Tests for LLMTaskGenerator implementation."""
    
    def setUp(self):
        """Set up mock LLM provider."""
        self.mock_llm = MagicMock(spec=ILLMProvider)
        
    def _create_mock_response(self, tasks):
        """Helper to create a mock LLM JSON response."""
        return json.dumps({"tasks": tasks})
    
    def test_generates_tasks_from_seed(self):
        """Should generate a list of tasks from a website seed."""
        from src.generators.task_generator import LLMTaskGenerator
        
        mock_tasks = [
            {"id": "task_1", "name": "Browse Products", 
             "description": "Find a product in the catalog",
             "steps": ["Navigate to home", "Click on category", "View product"]}
        ]
        self.mock_llm.prompt.return_value = self._create_mock_response(mock_tasks)
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce")
        tasks = generator.generate("online_bookstore", config)
        
        self.assertIsInstance(tasks, list)
        self.assertGreater(len(tasks), 0)
        
    def test_task_has_required_fields(self):
        """Each generated task should have id, name, description, steps."""
        from src.generators.task_generator import LLMTaskGenerator
        
        mock_tasks = [
            {"id": "task_1", "name": "Add to Cart", 
             "description": "Add an item to shopping cart",
             "steps": ["Find product", "Click add to cart", "View cart"]}
        ]
        self.mock_llm.prompt.return_value = self._create_mock_response(mock_tasks)
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce")
        tasks = generator.generate("online_bookstore", config)
        
        task = tasks[0]
        self.assertTrue(hasattr(task, 'id'))
        self.assertTrue(hasattr(task, 'name'))
        self.assertTrue(hasattr(task, 'description'))
        self.assertTrue(hasattr(task, 'steps'))
        
    def test_respects_step_count_range(self):
        """Tasks should have steps within the configured range."""
        from src.generators.task_generator import LLMTaskGenerator
        
        mock_tasks = [
            {"id": "task_1", "name": "Checkout", 
             "description": "Complete purchase",
             "steps": ["Add item", "Go to cart", "Enter address", "Pay", "Confirm"]}
        ]
        self.mock_llm.prompt.return_value = self._create_mock_response(mock_tasks)
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce", min_steps=3, max_steps=8)
        tasks = generator.generate("online_bookstore", config)
        
        for task in tasks:
            self.assertGreaterEqual(len(task.steps), 3)
            self.assertLessEqual(len(task.steps), 8)
            
    def test_returns_list_of_tasks(self):
        """Should return a list, even if empty."""
        from src.generators.task_generator import LLMTaskGenerator
        
        self.mock_llm.prompt.return_value = json.dumps({"tasks": []})
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce")
        tasks = generator.generate("online_bookstore", config)
        
        self.assertIsInstance(tasks, list)
        
    def test_uses_correct_prompt(self):
        """Should use PROMPT_TASK_GENERATION from library."""
        from src.generators.task_generator import LLMTaskGenerator
        from src.prompts.library import PROMPT_TASK_GENERATION
        
        self.mock_llm.prompt.return_value = self._create_mock_response([])
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce", task_count_min=3, task_count_max=5, min_steps=4, max_steps=6)
        generator.generate("online_bookstore", config)
        
        # Verify prompt was called with formatted template
        call_args = self.mock_llm.prompt.call_args[0][0]
        self.assertIn("e-commerce", call_args)
        self.assertIn("4", call_args)  # min_steps
        self.assertIn("6", call_args)  # max_steps
        
    def test_handles_malformed_llm_response(self):
        """Should handle malformed JSON gracefully."""
        from src.generators.task_generator import LLMTaskGenerator
        
        self.mock_llm.prompt.return_value = "not valid json"
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce")
        tasks = generator.generate("online_bookstore", config)
        
        self.assertIsInstance(tasks, list)
        self.assertEqual(len(tasks), 0)
        
    def test_handles_missing_fields(self):
        """Should provide defaults for missing task fields."""
        from src.generators.task_generator import LLMTaskGenerator
        
        # LLM returns task with missing 'name' field
        mock_tasks = [{"id": "task_1", "description": "Do something", "steps": ["step1"]}]
        self.mock_llm.prompt.return_value = self._create_mock_response(mock_tasks)
        
        generator = LLMTaskGenerator(self.mock_llm)
        config = TaskConfig(website_type="e-commerce")
        tasks = generator.generate("online_bookstore", config)
        
        # Strict validation should reject this now
        self.assertEqual(len(tasks), 0)


class TestTaskConfig(unittest.TestCase):
    """Tests for TaskConfig dataclass."""
    
    def test_default_values(self):
        """TaskConfig should have sensible defaults."""
        config = TaskConfig(website_type="e-commerce")
        self.assertEqual(config.task_count_min, 3)
        self.assertEqual(config.task_count_max, 7)
        self.assertEqual(config.min_steps, 3)
        self.assertEqual(config.max_steps, 8)
        
    def test_custom_values(self):
        """TaskConfig should accept custom values."""
        config = TaskConfig(
            website_type="fitness_tracker",
            task_count_min=5,
            task_count_max=10,
            min_steps=5,
            max_steps=12
        )
        self.assertEqual(config.task_count_min, 5)
        self.assertEqual(config.max_steps, 12)


if __name__ == '__main__':
    unittest.main()
